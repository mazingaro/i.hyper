<h2>DESCRIPTION</h2>

<p><em>i.hyper.preproc</em> performs preprocessing of hyperspectral data stored as a 3D raster map (<code>raster_3d</code>).
It is designed to improve data quality, suppress noise, and transform the spectral dimension into representations
better suited for scientific analysis and machine learning workflows.</p>

<p>The module operates directly on hyperspectral cubes imported with
<a href="i.hyper.import.html">i.hyper.import</a> or other compatible 3D raster datasets.
All transformations are performed along the spectral (<em>z</em>) dimension for each spatial position (<em>x, y</em>).
<p>Preprocessing steps can be chained together in a pipeline, specified with the <code>steps</code> option.
Each stage is executed sequentially according to the defined preprocessing pipeline.
The module displays the full pipeline sequence in the console
(for example: <code>Savitzky–Golay → Baseline correction → Continuum removal → PCA</code>),
providing a clear overview of the operations applied in order.</p>


<p><em>i.hyper.preproc</em> is part of the <b>i.hyper</b> module family and provides a reproducible, modular
framework for spectral preprocessing prior to feature extraction, classification, or regression.
All output maps are 3D rasters (<code>raster_3d</code>) compatible with the rest of the <em>i.hyper</em> suite.</p>

<h2>FUNCTIONALITY</h2>

<p>The following preprocessing methods are supported:</p>

<ul>
  <li><b>Savitzky–Golay (sav_gol)</b> – Polynomial smoothing and derivative computation to reduce spectral noise and enhance absorption features.</li>
  <li><b>Baseline correction (baseline)</b> – Removes global trends or offsets in reflectance curves.</li>
  <li><b>Continuum removal (cont_rem)</b> – Normalizes spectra to their convex hull to highlight relative absorption depths.</li>
  <li><b>Principal Component Analysis (pca)</b> – Linear dimensionality reduction using eigen decomposition of covariance.</li>
  <li><b>Kernel PCA (kpca)</b> – Nonlinear dimensionality reduction using kernel functions (RBF, polynomial, sigmoid).</li>
  <li><b>Nystroem approximation (nystroem)</b> – Scalable approximation of Kernel PCA using a low-rank kernel mapping followed by PCA compression. Provides nonlinear feature extraction suitable for large hyperspectral cubes.</li>
  <li><b>Fast Independent Component Analysis (fastica)</b> – Separates statistically independent spectral sources or mixtures.</li>
  <li><b>Truncated Singular Value Decomposition (tsvd)</b> – Linear dimensionality reduction preserving dominant singular vectors (useful for sparse data).</li>
  <li><b>Non-negative Matrix Factorization (nmf)</b> – Decomposes spectra into additive non-negative basis components.</li>
  <li><b>Sparse Principal Component Analysis (sparsepca)</b> – PCA variant enforcing sparsity on component loadings for interpretability.</li>
</ul>

<p>Multiple steps can be combined in one command by listing them in <code>steps=</code> (comma-separated).
For example, <code>steps='sav_gol,baseline,cont_rem,kpca'</code> will execute all four in sequence.
Intermediate rasters are handled internally and automatically cleaned up.</p>

<p>All dimensionality reduction methods are implemented using the
<a href="https://scikit-learn.org/stable/api/sklearn.decomposition.html">scikit-learn</a> library.
For detailed algorithmic descriptions and parameter explanations, refer to the official
scikit-learn documentation.</p>

<h2>NOTES</h2>

<p>The module is constructed as a preprocessing pipeline engine.
Each transformation acts spectrally while preserving full spatial alignment.
Operations are reported in the console as a sequential pipeline.</p>

<p>When using PCA, KPCA, FastICA, NMF, or SparsePCA, the number of output components can be controlled using the
<code>dr_components</code> parameter.</p>

<p><b>Chunked dimensionality reduction:</b><br>
Large hyperspectral datasets can be processed in smaller portions using the <code>dr_chunk_size</code> option.
This enables dimensionality reduction on datasets exceeding system memory capacity.
When <code>dr_chunk_size</code> is used with kernel-based methods (e.g., <b>KPCA</b>),
the algorithm operates as an <em>approximation</em> of the full kernel mapping,
trading some precision for scalability.</p>

<p><b>Model export and reuse:</b><br>
Trained dimensionality reduction models can be exported using the <code>dr_export</code> option.
The exported model (in <code>.pkl</code> format) can be reused to transform other spectra—such as field or laboratory
measurements from a spectroradiometer—into the same reduced feature space.
This allows consistent feature alignment between image-derived data and point spectra,
facilitating integrated machine learning and spectral modeling workflows.</p>

<p>Results can be directly used by <em>i.hyper.explore</em>, <em>i.hyper.composite</em>, or exported with
<em>i.hyper.export</em> for further analysis.</p>

<h2>EXAMPLES</h2>

<div class="code"><pre>
# Example 1: Savitzky–Golay smoothing (basic denoising)

# Set the region
g.region raster_3d=prisma

# Perform Savitzky–Golay smoothin with a window of 7 bands and polynomial order of 3
i.hyper.preproc input=prisma output=prisma_savgol \
                window_length=7 polyorder=3

# Console output:
Savitzky–Golay
Loading floating point  data with 4  bytes ...  (1254x1222x234)
(Fri Nov  5 13:12:00 2025) Command finished (7 min 13 sec) 
</pre></div>

<div class="code"><pre>
# Example 2: PCA transformation

# Set the region
g.region raster_3d=enmap

# Performs PCA
# Interpolaties missing values in valid bands
i.hyper.preproc input=enmap output=enmap_pca \
                dr_method=pca dr_components=10 -q

# Console output:
PCA
Interpolating missing values across spectral bands...
Loading floating point  data with 4  bytes ...  (1263x1127x10)
(Fri Nov  5 13:12:00 2025) Command finished (1 min 45 sec) 
</pre></div>

<div class="code"><pre>
# Example 3.1: Combined preprocessing pipeline

# Set the region
g.region raster_3d=tanager

# Savitzky–Golay derivative + baseline correction + continuum removal + Nystroem
# Interpolaties missing values in valid bands
# Processes the hyperspectral 3D map in chunks and exports the fitted Nystroem model
i.hyper.preproc input=tanager output=tanager_ml \
                polyorder=3 derivative_order=1 window_length=9 \
                -b -c -q \
                dr_method=nystroem dr_components=30 \
                dr_chunk_size=5000 \
                dr_export=/models/tanager_nystroem.pkl

# Console output:
Savitzky–Golay → Baseline correction → Continuum removal → NYSTROEM
Interpolating missing values across spectral bands...
Loading floating point  data with 4  bytes ...  (1254x1222x30)
(Fri Nov  5 13:12:00 2025) Command finished (13 min 59 sec)   
</pre></div>

<div class="code"><pre>
# Example 3.2: Using the exported Nystroem model in Python
import joblib, numpy as np

# Load exported Nystroem model (kernel map + PCA compressor)
feature_map, pca_after = joblib.load("/models/tanager_nystroem.pkl")

# Load new field spectra (rows = samples, cols = wavelengths
# The spectra must use the same wavelength order and scaling as the hyperspectral 3D map)
spectra = np.loadtxt("/data/field_spectra.txt")

# Apply the same nonlinear mapping and dimensionality reduction
Z = feature_map.transform(spectra)
spectra_reduced = pca_after.transform(Z)
</pre></div>

<h2>SEE ALSO</h2>

<a href="i.hyper.explore.html">i.hyper.explore</a>,
<a href="i.hyper.composite.html">i.hyper.composite</a>,
<a href="i.hyper.export.html">i.hyper.export</a>,
<a href="i.hyper.import.html">i.hyper.import</a>,
<a href="https://grass.osgeo.org/grass-stable/manuals/r3.stats.html">r3.stats</a>
<a href="https://grass.osgeo.org/grass-stable/manuals/r3.univar.html">r3.stats</a>


<h2>DEPENDENCIES</h2>

<ul>
  <li><b>NumPy</b> – Core numerical operations and array manipulation.</li>
  <li><b>SciPy</b> – Signal processing.</li>
  <li><b>scikit-learn</b> – Machine learning algorithms for PCA, KPCA, FastICA, NMF, SparsePCA, TruncatedSVD, and Nystroem.</li>
</ul>

<h2>AUTHORS</h2>

Alen Mangafić and Tomaž Žagar, Geodetic Institute of Slovenia
